# ETL-Pipeline-for-Contextual-RAG-based-Question-Answering

This project implements an end-to-end ETL pipeline for building a Retrieval Augmented Generation (RAG) system.
The pipeline extracts unstructured text data, preprocesses and chunks it for LLM consumption, generates semantic embeddings, stores them in a vector database (FAISS), and retrieves relevant context to generate grounded responses using a lightweight LLM.

The system is designed to be fully runnable, explainable, and debuggable, focusing on practical AI engineering rather than theoretical concepts.

ğŸ—ï¸ System Architecture
Raw Text Data
   â†“
Text Cleaning & Normalization (Transform)
   â†“
Chunking with Overlap
   â†“
Embedding Generation (SentenceTransformers)
   â†“
Vector Storage (FAISS)
   â†“
Semantic Retrieval
   â†“
LLM Response Generation (RAG)

âš™ï¸ Tech Stack

Language: Python

Embeddings: SentenceTransformers

Vector Database: FAISS

LLM: Hugging Face Transformers (FLAN-T5)

Numerical Processing: NumPy
